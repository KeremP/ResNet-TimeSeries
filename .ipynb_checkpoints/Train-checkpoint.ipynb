{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fatal-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from purgedSplit import PurgedGroupTimeSeriesSplit\n",
    "from earlystopping import EarlyStopping\n",
    "\n",
    "from Resnet import Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distant-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'numerai_dataset_256/numerai_training_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "located-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "EPOCHS = 200\n",
    "LR = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sweet-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data\n",
    "train_data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "#Dataset is large, lets quick and dirty optimize memory\n",
    "train_data = train_data.astype({c: np.float32 for c in train_data.select_dtypes(include='float64').columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thorough-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [c for c in train_data.columns if 'feature' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "willing-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wired-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom dataset - used in JS comp.\n",
    "class MarketDataset:\n",
    "    def __init__(self, df):\n",
    "        self.features = df[feat_cols].values\n",
    "        self.label = df[target_cols].values.reshape(-1,len(target_cols))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float)\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entertaining-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract era number from Era string\n",
    "train_data['erano']=train_data.era.str.slice(3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "considerable-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test hold-out\n",
    "train_set = train_data.query('erano < 100').reset_index(drop=True)\n",
    "test_set = train_data.query('erano > 100').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "rolled-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fresh-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function\n",
    "def train_fn(model, optimizer, eras, train_dataset, loss_fn, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for era in eras:\n",
    "        df = train_dataset[train_dataset.era==era]\n",
    "        X,y = torch.from_numpy(df[feat_cols].values).float().to(device),torch.from_numpy(df[target_cols].values).float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "    final_loss/=len(eras)\n",
    "        \n",
    "    return final_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spoken-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference function - outputs val_loss and epoch predictions for eval\n",
    "def inference(model, eras, val_dataset, device,loss_fn=None):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    val_loss = 0\n",
    "    for era in eras:\n",
    "        df = val_dataset[val_dataset.era==era]\n",
    "        X,y = torch.from_numpy(df[feat_cols].values).float().to(device),torch.from_numpy(df[target_cols].values).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X)\n",
    "        if loss_fn:\n",
    "            loss = loss_fn(outputs,y)\n",
    "        \n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "        \n",
    "        \n",
    "    preds = np.concatenate(preds).reshape(-1,len(target_cols))\n",
    "    \n",
    "    if loss_fn:\n",
    "        val_loss/=len(eras)\n",
    "    else:\n",
    "        val_loss = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    return val_loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "equivalent-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#purged group time series split - prevents leakage from trainig\n",
    "#to val sets.\n",
    "#5 splits \"embargo\"/group gap of 20. Group by era.\n",
    "gkf = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=20)\n",
    "splits = list(gkf.split(train_set['target'],groups=train_set['erano'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "#todo - refactor this to work for new architecture\n",
    "for _fold, (tr,te) in enumerate(splits):\n",
    "    print(f'Fold: {_fold}')\n",
    "    seed_everything(seed=1111+_fold)\n",
    "    \n",
    "    model = Resnet(len(feat_cols),len(target_cols),[0.19856,0.23423,0.15234,0.18923,0.20213])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)\n",
    "    #optimizer = FTML(model.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,base_lr=LR,max_lr=3e-2,cycle_momentum=False)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    #loss_fn = SmoothBCEwLogits(smoothing=0.005)\n",
    "    \n",
    "    model_weights = f'{CACHE_PATH}online_model_{_fold}_v2.pkl'\n",
    "    es = EarlyStopping(patience=10,mode='min')\n",
    "    \n",
    "\n",
    "    train_dataset = train_set.loc[tr]\n",
    "    valid_dataset = train_set.loc[te]\n",
    "    train_eras = train_dataset.era.unique()\n",
    "    valid_eras = valid_dataset.era.unique()\n",
    "    np.random.shuffle(train_eras)\n",
    "    np.random.shuffle(valid_eras)\n",
    "#     train_dataset = MarketDataset(train_set.loc[tr])\n",
    "#     valid_dataset = MarketDataset(train_set.loc[te])\n",
    "#     train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "#     valid_loader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "    \n",
    "    for epoch in (t:=trange(EPOCHS)):\n",
    "        train_loss = train_fn(model,optimizer,train_eras,train_dataset,loss_fn,device)\n",
    "        scheduler.step()\n",
    "        valid_loss, valid_preds = inference(model,valid_eras,valid_dataset,device,loss_fn)\n",
    "        \n",
    "        #roc_score = roc_auc_score(train_set.loc[te][target_cols].values,valid_preds)\n",
    "        \n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "        \n",
    "        es(valid_loss,model,model_path=model_weights)\n",
    "        if es.early_stop:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "        t.set_description('Train loss {} Valid loss {}'.format(train_loss,valid_loss))\n",
    "    #torch.save(model.state_dict(),f'{CACHE_PATH}model_{_fold}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
